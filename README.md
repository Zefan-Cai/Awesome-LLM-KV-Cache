
<!-- ![llm-kv-cache](https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache/assets/31974251/4d9ab775-f200-471d-a289-e2b14296b633) -->

<!-- <div align='center'>
  <img src=https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg >
  <img src=https://img.shields.io/github/downloads/Zefan-Cai/Awesome-LLM-KV-Cache/total?color=ccf&label=downloads&logo=github&logoColor=lightgrey >
  <img src=https://img.shields.io/github/forks/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/stars/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/github/watchers/Zefan-Cai/Awesome-LLM-KV-Cache.svg?style=social >
  <img src=https://img.shields.io/badge/Release-v1.6-brightgreen.svg >
  <img src=https://img.shields.io/badge/License-GPLv3.0-turquoise.svg >
 </div>    -->

## ğŸ“’Introduction
Awesome-LLM-KV-Cache: A curated list of [ğŸ“™Awesome LLM KV Cache Papers with Codes](#paperlist). This repository is for personal use of learning and classifying the burning KV Cache related papers!

## Â©ï¸Citations 


## ğŸ“–Contents 
<div id="paperlist"></div>

* ğŸ“–[Trending Inference Topics](#Trending-Inference-Topics)ğŸ”¥ğŸ”¥ğŸ”¥
* ğŸ“–[KV Cache Compression](#KV-Cache-Compression)ğŸ”¥ğŸ”¥
* ğŸ“–[KV Cache Merge](#KV-Cache-Merge)ğŸ”¥ğŸ”¥
* ğŸ“–[Budget Allocation](#Budget-Allocation)ğŸ”¥
* ğŸ“–[Cross-Layer KV Cache Utilization](#Cross-Layer-KV-Cache-Utilization)ğŸ”¥
* ğŸ“–[KV Cache Quantization](#KV-Cache-Quantization)ğŸ”¥
* ğŸ“–[Low-Rank KV Cache Decomposition](#Low-Rank-KV-Cache-Decomposition)ğŸ”¥
* ğŸ“–[Observation](#Observation)ğŸ”¥ğŸ”¥
* ğŸ“–[Evaluation](#Evaluation)ğŸ”¥
* ğŸ“–[Systems](#Systems)
* ğŸ“–[Others](#Others)

### ğŸ“–Trending Inference Topics ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Trending-Inference-Topics"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.05| ğŸ”¥ğŸ”¥ğŸ”¥[DeepSeek-V2] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model(@DeepSeek-AI)|[[pdf]](https://arxiv.org/pdf/2405.04434) | [[DeepSeek-V2]](https://github.com/deepseek-ai/DeepSeek-V2) ![](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2.svg?style=social)| â­ï¸â­ï¸â­ï¸ | 
|2024.05|ğŸ”¥ğŸ”¥ğŸ”¥[YOCO] You Only Cache Once: Decoder-Decoder Architectures for Language Models(@Microsoft)| [[pdf]](https://arxiv.org/pdf/2405.05254) | [[unilm-YOCO]](https://github.com/microsoft/unilm/tree/master/YOCO) ![](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social) |â­ï¸â­ï¸â­ï¸ |  
|2024.06|ğŸ”¥ğŸ”¥[**Mooncake**] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving(@Moonshot AI) |[[pdf]](https://github.com/kvcache-ai/Mooncake/blob/main/Mooncake-v1.pdf) | [[Mooncake]](https://github.com/kvcache-ai/Mooncake) ![](https://img.shields.io/github/stars/kvcache-ai/Mooncake.svg?style=social)|â­ï¸â­ï¸â­ï¸ |    
|2024.07|ğŸ”¥ğŸ”¥ğŸ”¥[**FlashAttention-3**] FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision(@TriDao etc) |[[pdf]](https://tridao.me/publications/flash3/flash3.pdf)|[[flash-attention]](https://github.com/Dao-AILab/flash-attention) ![](https://img.shields.io/github/stars/Dao-AILab/flash-attention.svg?style=social)|â­ï¸â­ï¸â­ï¸ |  
|2024.07|ğŸ”¥ğŸ”¥ğŸ”¥[**MInference 1.0**] MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention(@Microsoft) |[[pdf]](https://arxiv.org/pdf/2407.02490)|[[MInference 1.0]](https://github.com/microsoft/MInference) ![](https://img.shields.io/github/stars/microsoft/MInference.svg?style=social)|â­ï¸â­ï¸â­ï¸ |

### LLM KV Cache Compression ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))
<div id="#KV-Cache-Compression"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.06| ğŸ”¥ğŸ”¥[**H2O**] H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models|[[pdf]](https://arxiv.org/abs/2306.14048) | [[H2O]](https://github.com/FMInference/H2O) ![](https://img.shields.io/github/stars/FMInference/H2O.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Attention-based selection|
|2023.09| ğŸ”¥ğŸ”¥ğŸ”¥[**StreamingLLM**] Efficient Streaming Language Models with Attention Sinks|[[pdf]](https://arxiv.org/abs/2309.17453) | [[streaming-llm]](https://github.com/mit-han-lab/streaming-llm) ![](https://img.shields.io/github/stars/mit-han-lab/streaming-llm.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Retain first few tokens|
|2023.10| ğŸ”¥[**FastGen**] Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs|[[pdf]](https://arxiv.org/abs/2310.01801) | | â­ï¸â­ï¸ |Head-specific compression strategies|
|2023.10| ğŸ”¥ğŸ”¥[**CacheGen**] KV Cache Compression and Streaming for Fast Large Language Model Serving|[[pdf]](https://arxiv.org/abs/2310.07240) | [[LMCache]](https://github.com/LMCache/LMCache) ![](https://img.shields.io/github/stars/LMCache/LMCache.svg?style=social) | â­ï¸â­ï¸â­ï¸ |Compress KV cache to bitstreams for storage and sharing|
|2024.04| ğŸ”¥ğŸ”¥[**SnapKV**] SnapKV: LLM Knows What You are Looking for Before Generation|[[pdf]](https://arxiv.org/abs/2404.14469) | [[SnapKV]](https://github.com/FasterDecoding/SnapKV) ![](https://img.shields.io/github/stars/FasterDecoding/SnapKV.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Attention Pooling before selection|
|2024.05| [**Scissorhands**] Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time|[[pdf]](https://arxiv.org/abs/2305.17118) | | â­ï¸ ||
|2024.06| ğŸ”¥A Simple and Effective L2 Norm-Based Strategy for KV Cache Compression|[[pdf]](https://arxiv.org/abs/2406.11430) | | â­ï¸ |L2 Norm is better than attention as a metrics||
|2024.06| CORM: Cache Optimization with Recent Message for Large Language Model Inference|[[pdf]](https://arxiv.org/pdf/2404.15949) | | â­ï¸ |||
|2024.07| Efficient Sparse Attention needs Adaptive Token Release|[[pdf]](https://arxiv.org/abs/2407.02328) | | â­ï¸ ||
|2024.03| [**ALISA**] ALISA: Accelerating Large Language Model Inference via Sparsity-Aware KV Caching|[[pdf]](https://arxiv.org/abs/2403.17312) | | â­ï¸ |
|2024.03| ğŸ”¥ğŸ”¥ğŸ”¥[**FastV**] Code for paper: An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models|[[pdf]](https://arxiv.org/abs/2403.06764) | [[EasyKV]](https://github.com/pkunlp-icler/FastV) ![](https://img.shields.io/github/stars/pkunlp-icler/FastV.svg?style=social)| â­ï¸â­ï¸â­ï¸ |
|2024.03| [**Keyformer**] Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference|[[pdf]](https://arxiv.org/abs/2403.09054) | [[keyformer-llm]](https://github.com/d-matrix-ai/keyformer-llm) ![](https://img.shields.io/github/stars/d-matrix-ai/keyformer-llm.svg?style=social)| â­ï¸â­ï¸ |
|2024.06| Effectively Compress KV Heads for LLM |[[pdf]](https://arxiv.org/abs/2406.07056) | | â­ï¸ |
|2024.06| ğŸ”¥ Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters|[[pdf]](https://arxiv.org/abs/2406.12335) | | â­ï¸ |
|2024.06|  On the Efficacy of Eviction Policy for Key-Value Constrained Generative Language Model Inference|[[pdf]](https://arxiv.org/abs/2406.12335) | [[EasyKV]](https://github.com/DRSY/EasyKV) ![](https://img.shields.io/github/stars/DRSY/EasyKV.svg?style=social)| â­ï¸ |



### KV Cache Merge ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))
<div id="KV-Cache-Merge"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.10| ğŸ”¥ğŸ”¥[**CacheBlend**] Fast Large Language Model Serving for RAG with Cached Knowledge Fusion|[[pdf]](https://arxiv.org/abs/2405.16444) | [[LMCache]](https://github.com/LMCache/LMCache) ![](https://img.shields.io/github/stars/LMCache/LMCache.svg?style=social) | â­ï¸â­ï¸â­ï¸ |Selective update when merging KV caches|
|2023.12| ğŸ”¥ Compressed Context Memory For Online Language Model Interaction|[[pdf]](https://arxiv.org/pdf/2312.03414) | [[ContextMemory]](https://github.com/snu-mllab/Context-Memory) ![](https://img.shields.io/github/stars/snu-mllab/Context-Memory.svg?style=social) | â­ï¸â­ï¸â­ï¸ |Finetuning language models to recurrently compress KV caches|
|2024.01| [**CaM**] CaM: Cache Merging for Memory-efficient LLMs Inference|[[pdf]](https://openreview.net/forum?id=LCTmppB165) | [[cam]](https://github.com/zyxxmu/cam) ![](https://img.shields.io/github/stars/zyxxmu/cam.svg?style=social)| â­ï¸â­ï¸ |
|2024.05| ğŸ”¥ğŸ”¥ You Only Cache Once: Decoder-Decoder Architectures for Language Models|[[pdf]](https://arxiv.org/abs/2405.05254) | [[unilm]](https://github.com/microsoft/unilm) ![](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social)| â­ï¸â­ï¸ |
|2024.06| ğŸ”¥ğŸ”¥[**D2O**] D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models|[[pdf]](https://arxiv.org/abs/2406.13035) | | â­ï¸â­ï¸â­ï¸ |
|2024.07| ğŸ”¥ [**KVMerger**]Model Tells You Where to Merge: Adaptive KV Cache Merging for LLMs on Long-Context Tasks|[[pdf]](https://arxiv.org/abs/2407.08454) | | â­ï¸â­ï¸â­ï¸ |

### Budget Allocation ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))
<div id="Budget-Allocation"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.05| ğŸ”¥[**PyramidInfer**] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference|[[pdf]](https://arxiv.org/abs/2405.12532) | [[PyramidInfer]](https://github.com/mutonix/pyramidinfer) ![](https://img.shields.io/github/stars/mutonix/pyramidinfer.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Layer-wise budget allocation|
|2024.06| ğŸ”¥[**PyramidKV**] PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling|[[pdf]](https://arxiv.org/abs/2406.02069) | [[PyramidKV]](https://github.com/Zefan-Cai/PyramidKV) ![](https://img.shields.io/github/stars/Zefan-Cai/PyramidKV.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Layer-wise budget allocation|
|2024.07| ğŸ”¥[**Ada-KV**] Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference|[[pdf]](https://arxiv.org/abs/2407.11550) | | â­ï¸â­ï¸â­ï¸ |Head-wise budget allocation|
|2024.07|  RazorAttention: Efficient KV Cache Compression Through Retrieval Heads|[[pdf]](https://arxiv.org/pdf/2407.15891) | | â­ï¸ |

### Cross-Layer KV Cache Utilization ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))
<div id="Cross-Layer-KV-Cache-Utilization"></div>

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.05| ğŸ”¥ Reducing Transformer Key-Value Cache Size with Cross-Layer Attention|[[pdf]](https://arxiv.org/abs/2405.12981) | | â­ï¸ ||
|2024.05| ğŸ”¥ Layer-Condensed KV Cache for Efficient Inference of Large Language Models|[[pdf]](https://arxiv.org/abs/2405.10637) | [[**LCKV**]](https://github.com/whyNLP/LCKV) ![](https://img.shields.io/github/stars/whyNLP/LCKV.svg?style=social)| â­ï¸â­ï¸ ||
|2024.05| ğŸ”¥ğŸ”¥ğŸ”¥[**MiniCache**] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models|[[pdf]](https://arxiv.org/abs/2405.14366) | | â­ï¸â­ï¸â­ï¸ ||
|2024.06| ğŸ”¥[**MLKV**] MLKV: Multi-Layer Key-Value Heads for Memory Efficient Transformer Decoding|[[pdf]](https://arxiv.org/abs/2406.09297) | [[pythia-mlkv]](https://github.com/zaydzuhri/pythia-mlkv) ![](https://img.shields.io/github/stars/zaydzuhri/pythia-mlkv.svg?style=social)| â­ï¸â­ï¸ ||

### KV Cache Quantization ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="KV-Cache-Quantization"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2023.03| ğŸ”¥[**GEAR**] GEAR: An Efficient KV Cache Compression Recipe for Near-Lossless Generative Inference of LLM|[[pdf]](https://arxiv.org/abs/203.05527) | [[GEAR]](https://github.com/opengear-project/GEAR) ![](https://img.shields.io/github/stars/opengear-project/GEAR.svg?style=social)| â­ï¸â­ï¸ ||
|2024.01| ğŸ”¥ğŸ”¥[**KVQuant**] KVQuant: Towards 10 Million Context Length LLM Inference with KV Cache Quantization|[[pdf]](https://arxiv.org/abs/2401.18079) | [[KVQuant]](https://github.com/SqueezeAILab/KVQuant) ![](https://img.shields.io/github/stars/SqueezeAILab/KVQuant.svg?style=social)| â­ï¸â­ï¸ |Make all KV cache quantized|
|2024.02| [No Token Left Behind] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization|[[pdf]](https://arxiv.org/abs/2402.18096) | | â­ï¸â­ï¸â­ï¸ ||
|2024.02| [**KIVI**] KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache|[[pdf]](https://arxiv.org/abs/2402.02750) | [[KIVI]](https://github.com/jy-yuan/KIVI) ![](https://img.shields.io/github/stars/jy-yuan/KIVI.svg?style=social)| â­ï¸â­ï¸ ||
|2024.02| [**WKVQuant**] WKVQuant: Quantizing Weight and Key/Value Cache for Large Language Models Gains More|[[pdf]](https://arxiv.org/pdf/2402.12065) ||||
|2024.03| [**QAQ**] QAQ: Quality Adaptive Quantization for LLM KV Cache|[[pdf]](https://arxiv.org/abs/2403.04643) | [[QAQ-KVCacheQuantization]](https://github.com/ClubieDong/QAQ-KVCacheQuantization) ![](https://img.shields.io/github/stars/ClubieDong/QAQ-KVCacheQuantization.svg?style=social)| â­ï¸ |attention-based KV cache quantized|
|2024.05| [**ZipCache**] ZipCache: Accurate and Efficient KV Cache Quantization with Salient Token Identification|[[pdf]](https://www.arxiv.org/abs/2405.14256) | | â­ï¸ ||
|2024.05|  Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression|[[pdf]](https://arxiv.org/abs/2405.12591) | | â­ï¸ ||
|2024.05| [**SKVQ**] SKVQ: Sliding-window Key and Value Cache Quantization for Large Language Models|[[pdf]](https://arxiv.org/abs/2405.06219) | [[SKVQ]](https://github.com/cat538/SKVQ) ![](https://img.shields.io/github/stars/cat538/SKVQ.svg?style=social)| â­ï¸ |
|2024.07| [**PQCache**] PQCache: Product Quantization-based KVCache for Long Context LLM Inference|[[pdf]](https://arxiv.org/abs/2407.12820) | | â­ï¸ ||


https://arxiv.org/abs/2402.12065
### Evaluation ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Evaluation"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.07| ğŸ”¥[Benchmark] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches|[[pdf]](https://arxiv.org/abs/2407.01527) | | â­ï¸ ||

### Low Rank KV Cache Decomposition ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Low-Rank-KV-Cache-Decomposition"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.02| Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference|[[pdf]](https://arxiv.org/abs/2402.09398) | [[LESS]](https://github.com/hdong920/LESS) ![](https://img.shields.io/github/stars/hdong920/LESS.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Fine-tune to make the KV cache low-ranked|
|2024.05| ğŸ”¥ğŸ”¥ğŸ”¥[**DeepSeek-V2**] DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model|[[pdf]](https://arxiv.org/abs/2405.04434) | [[DeepSeek-V2]](https://github.com/deepseek-ai/DeepSeek-V2) ![](https://img.shields.io/github/stars/deepseek-ai/DeepSeek-V2.svg?style=social)| â­ï¸â­ï¸â­ï¸ |Train low-rank KV cache from scratch|
|2024.06| [**Loki**] Loki: Low-Rank Keys for Efficient Sparse Attention|[[pdf]](https://arxiv.org/abs/2406.02542) | | â­ï¸ ||

### Observation ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Observation"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2022.09| In-context Learning and Induction Heads|[[pdf]](https://arxiv.org/abs/2209.11895) || â­ï¸â­ï¸ |
|2024.01| ğŸ”¥Transformers are Multi-State RNNs|[[pdf]](https://arxiv.org/abs/2401.06104) | [[TOVA]](https://github.com/schwartz-lab-NLP/TOVA) ![](https://img.shields.io/github/stars/schwartz-lab-NLP/TOVA.svg?style=social)| â­ï¸â­ï¸ |
|2024.04| ğŸ”¥[**Retrieval Head**] Retrieval Head Mechanistically Explains Long-Context Factuality|[[pdf]](https://arxiv.org/abs/2404.15574) | [[Retrieval_Head]](https://github.com/nightdessert/Retrieval_Head) ![](https://img.shields.io/github/stars/nightdessert/Retrieval_Head.svg?style=social)| â­ï¸â­ï¸â­ï¸ ||
|2024.04| ğŸ”¥[**Massive Activations**] Massive Activations in Large Language Models|[[pdf]](https://arxiv.org/pdf/2402.17762) | [[Massive Activation]](https://github.com/locuslab/massive-activations) ![](https://img.shields.io/github/stars/locuslab/massive-activations.svg?style=social)| â­ï¸â­ï¸â­ï¸ ||

### Systems ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Systems"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.06|ğŸ”¥ğŸ”¥[**Mooncake**] Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving(@Moonshot AI) |[[pdf]](https://github.com/kvcache-ai/Mooncake/blob/main/Mooncake-v1.pdf) | [[Mooncake]](https://github.com/kvcache-ai/Mooncake) ![](https://img.shields.io/github/stars/kvcache-ai/Mooncake.svg?style=social)|â­ï¸â­ï¸â­ï¸ |    
|2024.02| MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool |[[pdf]](https://arxiv.org/pdf/2406.17565) | | â­ï¸ ||

### Others ([Â©ï¸backğŸ‘†ğŸ»](#paperlist))  
<div id="Others"></div>  

|Date|Title|Paper|Code|Recom|Comment|
|:---:|:---:|:---:|:---:|:---:|:---:|
|2024.02| Effectively Compress KV Heads for LLM |[[pdf]](https://arxiv.org/pdf/2406.07056) | | â­ï¸ ||
|2024.07| ğŸ”¥ğŸ”¥Q-Sparse: All Large Language Models can be Fully Sparsely-Activated |[[pdf]](https://arxiv.org/abs/2407.10969) | [[GeneralAI]](https://aka.ms/GeneralAI) ![](https://img.shields.io/github/stars/microsoft/unilm.svg?style=social)| â­ï¸â­ï¸â­ï¸ ||



## Â©ï¸License  

GNU General Public License v3.0  

## ğŸ‰Contribute  

Welcome to star & submit a PR to this repo! 



```BibTeX
@misc{Awesome-LLM-Inference@2024,
  title={Awesome-LLM-KV-Cache: A curated list of Awesome LLM Inference Papers with codes},
  url={https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache},
  note={Open-source software available at https://github.com/Zefan-Cai/Awesome-LLM-KV-Cache},
  author={Zefan-Cai, etc},
  year={2024}
}
```
